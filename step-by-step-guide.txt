# Ressources: 
https://www.kdnuggets.com/2023/04/stepbystep-guide-web-scraping-python-beautiful-soup.html

1. Create folder
2. Create a requirements.txt file with all dependencies
3. Create virutalenv with anaconda: conda create --name webscraping_env python=3
4. Activate virtual environment: source activate webscraping_env
5. Install packages: pip install bs4, request OR
		     pip install -r requirements.txt
6. Initialise project with GitHub: git init
7. Create .gitignore file (list hidden files with ls -a): vim .gitignore, , esc-:wq, (nano .gitignore)

8. 



Steps, when setting up the project on another system:
1. Clone the git repository: git clone <repository_url>
2. cd project
3. Create and activate the Anaconda environment:
	conda create --name webscraping_env python=3
	conda activate webscraping_env
4. Install the project dependencies: pip install -r requirements.txt 
	(conda install --file requirements.txt)



TODOs:
Docker?