# Ressources: 
https://www.kdnuggets.com/2023/04/stepbystep-guide-web-scraping-python-beautiful-soup.html

1. Create folder
2. Create a requirements.txt file with all dependencies
3. Create virutalenv with anaconda: conda create --name webscraping_env python=3
4. Activate virtual environment: source activate webscraping_env
5. Install packages: pip install bs4, request OR
		     pip install -r requirements.txt
6. Initialise project with GitHub: git init
7. Create .gitignore file (list hidden files with ls -a): vim .gitignore, , esc-:wq, (nano .gitignore)

8. Push to git:
To link your local repository to your GitHub account and push your code to GitHub, you need to add a remote URL. Assuming you have already created a repository on GitHub, follow these steps:
	1. Create a new repository on GitHub (if you haven't already).
	2. Copy the repository URL (HTTPS or SSH)
	3. Terminal, navigate to the root directory of your project.
	4. Add the remote repository using the following command:
	 	git remote add origin https://github.com/anne-lene/webscraping.git
	5. Check: git remote -v


Steps, when setting up the project on another system:
1. Clone the git repository: git clone <repository_url>
2. cd project
3. Create and activate the Anaconda environment:
	conda create --name webscraping_env python=3
	conda activate webscraping_env
4. Install the project dependencies: pip install -r requirements.txt 
	(conda install --file requirements.txt)



TODOs:
Docker?